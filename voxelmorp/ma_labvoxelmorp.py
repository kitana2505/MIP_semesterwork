# -*- coding: utf-8 -*-
"""MA_LabVoxelMorp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zdiq7geB_FfHejzf0HlzJm_wk2m2eUpf

# Data Preparation
"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/MyDrive/MA_VoxelMorph/imgs.zip ./
!tar -xvzf ./imgs.zip

# install voxelmorph, which will also install dependencies: neurite and pystrum
!pip install voxelmorph

# imports
import os, sys

# third party imports
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2
from glob import glob
import os
from tqdm import tqdm
assert tf.__version__.startswith('2.'), 'This tutorial assumes Tensorflow 2.0+'

# local imports
import voxelmorph as vxm
import neurite as ne

def load_data(img_directory, resize_height=128):
  train_data = []

  # Get paths of all images
  data_path = glob(os.path.join(img_directory, "*.jpg"))

  # Read the first image to get height and width
  _frame = cv2.imread(data_path[0], cv2.IMREAD_GRAYSCALE)
  height, width = _frame.shape[:]

  # resize by the ratio of heigh / width
  resize_width = int(resize_height * width / height / 10) * 10

  # read, resize and append image
  for path in tqdm(data_path):
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    img_resize = cv2.resize(img, (resize_width, resize_height))
    train_data.append(img_resize)

  return np.array(train_data)

train_data = load_data("./imgs")

print(train_data.shape)

nb_vis = 5

# choose nb_vis sample indexes
idx = np.random.choice(train_data.shape[0], nb_vis, replace=False)
examples = [f for f in train_data[idx, ...]]

# plot
ne.plot.slices(examples, cmaps=['gray'], do_colorbars=True);

def train_test_split(dataset, val_per=0.05, seed=32):
  np.random.seed(seed)
  num_train = int(len(dataset) * (1-0.05))
  x_train = dataset[:num_train] / 255.0
  x_val = dataset[num_train:] / 255.0

  print(f"Training data: {len(x_train)} samples")
  print(f"Validation data: {len(x_val)} samples")

  return x_train, x_val

x_train, x_val = train_test_split(train_data)

"""# VoxelNet

"""

# build model using VxmDense
inshape = x_train.shape[1:]
# configure unet features
nb_features = [
    [32, 32, 32, 32],         # encoder features
    [32, 32, 32, 32, 32, 16]  # decoder features
]
vxm_model = vxm.networks.VxmDense(inshape, nb_features, int_steps=0)

print('input shape: ', ', '.join([str(t.shape) for t in vxm_model.inputs]))
print('output shape:', ', '.join([str(t.shape) for t in vxm_model.outputs]))

# voxelmorph has a variety of custom loss classes
losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss]

# usually, we have to balance the two losses by a hyper-parameter
lambda_param = 0.05
loss_weights = [1, lambda_param]

vxm_model.compile(optimizer='Adam', loss=losses, loss_weights=loss_weights)

def vxm_data_generator(x_data, batch_size=8):
    """
    Generator that takes in data of size [N, H, W], and yields data for
    our custom vxm model. Note that we need to provide numpy data for each
    input, and each output.

    inputs:  moving [bs, H, W, 1], fixed image [bs, H, W, 1]
    outputs: moved image [bs, H, W, 1], zero-gradient [bs, H, W, 2]
    """

    # preliminary sizing
    vol_shape = x_data.shape[1:] # extract data shape
    ndims = len(vol_shape)

    # prepare a zero array the size of the deformation
    # we'll explain this below
    zero_phi = np.zeros([batch_size, *vol_shape, ndims])

    while True:
        # prepare inputs:
        # images need to be of the size [batch_size, H, W, 1]
        idx1 = np.random.randint(0, x_data.shape[0], size=batch_size)
        moving_images = x_data[idx1, ..., np.newaxis]
        idx2 = np.random.randint(0, x_data.shape[0], size=batch_size)
        fixed_images = x_data[idx2, ..., np.newaxis]
        inputs = [moving_images, fixed_images]

        # prepare outputs (the 'true' moved image):
        # of course, we don't have this, but we know we want to compare
        # the resulting moved image with the fixed image.
        # we also wish to penalize the deformation field.
        outputs = [fixed_images, zero_phi]

        yield (inputs, outputs)

# let's test it
train_generator = vxm_data_generator(x_train)
in_sample, out_sample = next(train_generator)

# visualize
images = [img[0, :, :, 0] for img in in_sample + out_sample]
titles = ['moving', 'fixed', 'moved ground-truth (fixed)', 'zeros']
ne.plot.slices(images, titles=titles, cmaps=['gray'], do_colorbars=True);

nb_epochs = 100
steps_per_epoch = 100
hist = vxm_model.fit_generator(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2)

import matplotlib.pyplot as plt

def plot_history(hist, loss_name='loss'):
    # Simple function to plot training history.
    plt.figure()
    plt.plot(hist.epoch, hist.history[loss_name], '.-')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.show()

plot_history(hist)

# let's get some data
val_generator = vxm_data_generator(x_val, batch_size = 1)
for _ in range(5):
  val_input, empty_field = next(val_generator)

print(val_input[0].shape, val_input[1].shape)

val_pred = vxm_model.predict(val_input)

# visualize
images = [img[0, :, :, 0] for img in val_input + val_pred]
titles = ['moving', 'fixed', 'moved', 'flow']
cap = cv2.VideoCapture(0)

fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640,480))

cap = cv2.VideoCapture(0)

fourcc = cv2.VideoWriter_fourcc(*'XVID')
data = np.vstack((x_train, x_val))

fix_frame = data[0]
height, width = fix_frame.shape
out = cv2.VideoWriter('output.avi', fourcc, 20.0, (width,height))

for move_frame in data:
  data_input = [move_frame[np.newaxis, :, :, np.newaxis], fix_frame[np.newaxis, :, :, np.newaxis]]
  moved_frame, flow = vxm_model.predict(data_input)

  video_frame = cv2.cvtColor(moved_frame[0,:,:,0], cv2.COLOR_GRAY2BGR) * 255
  out.write(np.uint8(video_frame))

cap.release()
out.release()

